{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"SemEval2010_task8_training/TRAIN_FILE.TXT\")\n",
    "\n",
    "# read the train data line by line \n",
    "# store in a list\n",
    "def readinFile(f):\n",
    "    filelist = []\n",
    "    line = f.readline()\n",
    "    filelist.append(line)\n",
    "    while line:\n",
    "        line = f.readline()\n",
    "        filelist.append(line)\n",
    "    return filelist\n",
    "\n",
    "filelist = readinFile(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using 3 lists to store 3 elements\n",
    "# list_sentence: each sentence in the list\n",
    "# list_class: each class in the list\n",
    "# list comment: each comment in the list\n",
    "def separateFilelist(l):\n",
    "    list_sentence = []\n",
    "    list_class = []\n",
    "    list_comment = []\n",
    "    for i in range(len(l)):\n",
    "        if (i + 1) % 4 == 1:\n",
    "            list_sentence.append(l[i])\n",
    "        if (i + 1) % 4 == 2:\n",
    "            list_class.append(l[i])\n",
    "        if (i + 1) % 4 == 3:\n",
    "            list_comment.append(l[i])\n",
    "    # to remove the '' in the end, no use\n",
    "    list_sentence.remove('')\n",
    "    return list_sentence,list_class,list_comment\n",
    "\n",
    "list_sentence,list_class,list_comment = separateFilelist(filelist)\n",
    "\n",
    "# here we delete the sentences which give us noises\n",
    "# index: 212,2739,4218,4611,4783,6372\n",
    "\n",
    "list_sentence.remove(list_sentence[212])\n",
    "list_sentence.remove(list_sentence[2738])\n",
    "list_sentence.remove(list_sentence[4216])\n",
    "list_sentence.remove(list_sentence[4608])\n",
    "list_sentence.remove(list_sentence[4779])\n",
    "list_sentence.remove(list_sentence[6367])\n",
    "\n",
    "list_class.remove(list_class[212])\n",
    "list_class.remove(list_class[2738])\n",
    "list_class.remove(list_class[4216])\n",
    "list_class.remove(list_class[4608])\n",
    "list_class.remove(list_class[4779])\n",
    "list_class.remove(list_class[6367])\n",
    "\n",
    "list_comment.remove(list_comment[212])\n",
    "list_comment.remove(list_comment[2738])\n",
    "list_comment.remove(list_comment[4216])\n",
    "list_comment.remove(list_comment[4608])\n",
    "list_comment.remove(list_comment[4779])\n",
    "list_comment.remove(list_comment[6367])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the sentences is:  7994\n",
      "The length of the classes is:  7994\n",
      "The length of the comments is:  7994\n"
     ]
    }
   ],
   "source": [
    "def sizeoftrain(list_sentence,list_class,list_comment):\n",
    "    print(\"The length of the sentences is: \",len(list_sentence))\n",
    "    print(\"The length of the classes is: \",len(list_class))\n",
    "    print(\"The length of the comments is: \",len(list_comment))\n",
    "    \n",
    "sizeoftrain(list_sentence,list_class,list_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sirius/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /home/sirius/anaconda3/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def getNouns(list_sentence):\n",
    "    l_noun = []\n",
    "    for sentence in list_sentence:\n",
    "        flag = 0\n",
    "        l1 = ''\n",
    "        l2 = ''\n",
    "        couple_noun = []\n",
    "        for i in range(len(sentence)):\n",
    "            if (sentence[i] == '<') and (sentence[i+1] == 'e') and (sentence[i+2] == '1'):\n",
    "                flag = 1\n",
    "            if (sentence[i] == '>') and (sentence[i-1] == '1') and (sentence[i-2] == 'e') and (sentence[i-3] == '/'):\n",
    "                flag = 0\n",
    "            if (sentence[i] == '<') and (sentence[i+1] == 'e') and (sentence[i+2] == '2'):\n",
    "                flag = 2\n",
    "            if (sentence[i] == '>') and (sentence[i-1] == '2') and (sentence[i-2] == 'e') and (sentence[i-3] == '/'):\n",
    "                flag = 0\n",
    "\n",
    "            if flag == 1:\n",
    "                l1 += sentence[i]\n",
    "            if flag == 2:\n",
    "                l2 += sentence[i]\n",
    "        noun1_b = BeautifulSoup(l1)\n",
    "        noun2_b = BeautifulSoup(l2)\n",
    "        noun1 = noun1_b.get_text()\n",
    "        noun2 = noun2_b.get_text()\n",
    "        couple_noun.append(noun1)\n",
    "        couple_noun.append(noun2)\n",
    "        l_noun.append(couple_noun)\n",
    "    return l_noun\n",
    "\n",
    "l_noun = getNouns(list_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace the space in the marked nous into #, and replace these nous in the sentences\n",
    "# example: WANG Yaohui -> WANG#Yaohui\n",
    "\n",
    "import nltk.data\n",
    "import re\n",
    "\n",
    "def replace_the_space(l_noun):\n",
    "    l_new_noun = []\n",
    "    for couple in l_noun:\n",
    "        c = (couple[0].replace(' ','#'),couple[1].replace(' ','#'))\n",
    "        l_new_noun.append(c)\n",
    "    return l_new_noun\n",
    "\n",
    "def replace_original_noun(list_sentence,l_new_noun):\n",
    "    l_new_sentence = []\n",
    "    for sentence,couple in zip(list_sentence,l_new_noun):\n",
    "        left = couple[0].replace('#',' ')\n",
    "        right = couple[1].replace('#',' ')\n",
    "        sentence = re.sub(\"<e1>\" + left + \"</e1>\",couple[0],sentence)\n",
    "        sentence = re.sub(\"<e2>\" + right + \"</e2>\",couple[1],sentence)\n",
    "        l_new_sentence.append(sentence)\n",
    "    return l_new_sentence\n",
    "\n",
    "def get_the_new_sentence(list_sentence,l_noun):\n",
    "    final_sentence = []\n",
    "    \n",
    "    l_new_noun = replace_the_space(l_noun)\n",
    "    l_new_sentence = replace_original_noun(list_sentence,l_new_noun)\n",
    "    \n",
    "    for l in l_new_sentence:\n",
    "        sentence_text = re.sub(\"[^a-zA-Z#-]\",\" \",l)\n",
    "        words = sentence_text.lower().split()\n",
    "        final_sentence.append(words)\n",
    "    return final_sentence\n",
    "\n",
    "def get_the_final_couple(l_noun):\n",
    "    new_couple = replace_the_space(l_noun)\n",
    "    final_couple = []\n",
    "    for couple in new_couple:\n",
    "        c = (couple[0].lower(),couple[1].lower())\n",
    "        final_couple.append(c)\n",
    "    return final_couple\n",
    "\n",
    "# final sentences we need\n",
    "final_sentence = get_the_new_sentence(list_sentence,l_noun)\n",
    "# final marked nouns we need\n",
    "final_couple = get_the_final_couple(l_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_location(final_sentence,final_couple):\n",
    "    list_index = []\n",
    "    for couple,sentence in zip(final_couple,final_sentence):\n",
    "        #print(sentence)\n",
    "        #print(couple[0] + couple[1])\n",
    "        noun_couple = (sentence.index(couple[0]),sentence.index(couple[1]))\n",
    "        list_index.append(noun_couple)\n",
    "    return list_index\n",
    "\n",
    "# location of marked nouns in the sentences\n",
    "list_index = get_location(final_sentence, final_couple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-24 22:01:26,887: WARNING: consider setting layer size to a multiple of 4 for greater performance\n",
      "2017-01-24 22:01:26,887: INFO: collecting all words and their counts\n",
      "2017-01-24 22:01:26,888: INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-01-24 22:01:26,930: INFO: collected 19818 word types from a corpus of 136793 raw words and 7994 sentences\n",
      "2017-01-24 22:01:26,931: INFO: Loading a fresh vocabulary\n",
      "2017-01-24 22:01:27,026: INFO: min_count=1 retains 19818 unique words (100% of original 19818, drops 0)\n",
      "2017-01-24 22:01:27,026: INFO: min_count=1 leaves 136793 word corpus (100% of original 136793, drops 0)\n",
      "2017-01-24 22:01:27,082: INFO: deleting the raw counts dictionary of 19818 items\n",
      "2017-01-24 22:01:27,083: INFO: sample=0.0001 downsamples 330 most-common words\n",
      "2017-01-24 22:01:27,084: INFO: downsampling leaves estimated 79036 word corpus (57.8% of prior 136793)\n",
      "2017-01-24 22:01:27,084: INFO: estimated required memory for 19818 words and 50 dimensions: 17836200 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-24 22:01:27,140: INFO: resetting layer weights\n",
      "2017-01-24 22:01:27,358: INFO: training model with 8 workers on 19818 vocabulary and 50 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-01-24 22:01:27,359: INFO: expecting 7994 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-01-24 22:01:27,764: INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2017-01-24 22:01:27,773: INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2017-01-24 22:01:27,774: INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2017-01-24 22:01:27,790: INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2017-01-24 22:01:27,791: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2017-01-24 22:01:27,795: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-24 22:01:27,796: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-24 22:01:27,800: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-24 22:01:27,800: INFO: training on 683965 raw words (395178 effective words) took 0.4s, 916968 effective words/s\n",
      "2017-01-24 22:01:27,801: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# apply word2vec on our final sentences list\n",
    "from gensim import models\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s', level = logging.INFO)\n",
    "\n",
    "num_features = 50\n",
    "min_word_count = 1\n",
    "num_workers = 8\n",
    "context = 10\n",
    "downsampling = 0.0001\n",
    "print(\"Training model...\")\n",
    "\n",
    "model = models.word2vec.Word2Vec(final_sentence, workers = num_workers, size = num_features, min_count = min_word_count, window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-24 22:01:37,849: INFO: precomputing L2-norms of word weight vectors\n",
      "2017-01-24 22:01:37,966: INFO: saving Word2Vec object under wia, separately None\n",
      "2017-01-24 22:01:37,967: INFO: not storing attribute syn0norm\n",
      "2017-01-24 22:01:37,968: INFO: not storing attribute cum_table\n",
      "2017-01-24 22:01:38,074: INFO: saved wia\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace = True)\n",
    "\n",
    "model_name = \"wia\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'child'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man waman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# L1 L2 L3 L4\n",
    "def get_lexical_feature(final_couple,list_sentence,final_sentence):\n",
    "    lexical_feature = []\n",
    "    for couple,index,sentence in zip(final_couple,list_index,final_sentence):\n",
    "\n",
    "        noun1_v = model[couple[0]]\n",
    "        noun2_v = model[couple[1]]\n",
    "    \n",
    "        if index[0] == 0:\n",
    "            noun3_v = model[sentence[index[0]+1]] / 2 \n",
    "        if index[0] == (len(sentence)-1):\n",
    "            noun3_v = model[sentence[index[0]-1]] / 2\n",
    "        else:\n",
    "            noun3_v = (model[sentence[index[0]-1]] + model[sentence[index[0]+1]]) / 2\n",
    "        \n",
    "        if index[1] == 0:\n",
    "            noun4_v = model[sentence[index[1]+1]] / 2 \n",
    "        if index[1] == (len(sentence)-1):\n",
    "            noun4_v = model[sentence[index[1]-1]] / 2\n",
    "        else:\n",
    "            noun4_v = (model[sentence[index[1]-1]] + model[sentence[index[1]+1]]) / 2\n",
    "    \n",
    "        word_feature = np.concatenate((noun1_v,noun2_v,noun3_v,noun4_v))\n",
    "        lexical_feature.append(word_feature)\n",
    "    return np.array(lexical_feature)\n",
    "\n",
    "lexical_feature = get_lexical_feature(final_couple,list_sentence,final_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7994, 200)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# padding make all the sentences be the same length, using </s> to padding \n",
    "def pad_sentences(sentences, padding_word=\"</s>\"):\n",
    "    padding_word = \"</s>\"\n",
    "    sequence_length = max(len(x) for x in sentences) # get the max length in the dataset\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences\n",
    "\n",
    "sentences_padded = pad_sentences(final_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# define the beginning and the end \n",
    "xs = np.zeros(50)\n",
    "xe = np.zeros(50)\n",
    "pad = np.zeros(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the [WF, PF]\n",
    "def get_the_sentence_feature(sentences_padded,list_index):\n",
    "    window = 3\n",
    "    list_sentence_feature = []\n",
    "    for sentence,position in zip(sentences_padded,list_index):\n",
    "        wf_l = []\n",
    "        for index in range(len(sentence)):\n",
    "            if index == 0:\n",
    "                if sentence[index] == '</s>':\n",
    "                    a = pad\n",
    "                else:\n",
    "                    a = model[sentence[index]]\n",
    "                if sentence[index+1] == '</s>':\n",
    "                    b = pad\n",
    "                else:\n",
    "                    b = model[sentence[index+1]]\n",
    "                wf = np.concatenate((xs, a, b, [index-position[0]], [index-position[1]]))\n",
    "            if index == (len(sentence)-1):\n",
    "                if sentence[index-1] == '</s>':\n",
    "                    a = pad\n",
    "                else:\n",
    "                    a = model[sentence[index-1]]\n",
    "                if sentence[index] == '</s>':\n",
    "                    b = pad\n",
    "                else:\n",
    "                    b = model[sentence[index]]\n",
    "                wf = np.concatenate((a, b, xe, [index-position[0]], [index-position[1]]))\n",
    "            else:\n",
    "                if sentence[index-1] == '</s>':\n",
    "                    a = pad\n",
    "                else:\n",
    "                    a = model[sentence[index-1]]\n",
    "                if sentence[index] == '</s>':\n",
    "                    b = pad\n",
    "                else:\n",
    "                    b = model[sentence[index]]\n",
    "                if sentence[index+1] == '</s>':\n",
    "                    c = pad\n",
    "                else:\n",
    "                    c = model[sentence[index+1]]\n",
    "                wf = np.concatenate((a, b, c, [index-position[0]], [index-position[1]]))\n",
    "            wf_l.append(wf)\n",
    "        list_sentence_feature.append(np.array(wf_l))\n",
    "    return list_sentence_feature\n",
    "\n",
    "list_sentence_feature = get_the_sentence_feature(sentences_padded,list_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(list_sentence_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0],1,86,152)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7994, 1, 86, 152)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: Unable to get the number of gpus available: unknown error)\n",
      "2017-01-24 22:16:18,188: WARNING: CUDA is installed, but device gpu is not available  (error: Unable to get the number of gpus available: unknown error)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "import pandas as pd\n",
    "\n",
    "y_class = pd.get_dummies(np.array(list_class)).values.argmax(1)\n",
    "y = np_utils.to_categorical(y_class, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7994, 19)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model DCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling2D,MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.layers import Input,merge\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_input = Input(shape=(1,86,152))\n",
    "# conv1 = Convolution1D(32,3,activation='relu')(main_input)\n",
    "hidden1 = Dense(200,activation='linear')(main_input)\n",
    "pool = MaxPooling2D(pool_size = (1,200),dim_ordering='th')(hidden1)\n",
    "flatten = Flatten()(pool)\n",
    "hidden = Dense(100,activation='tanh')(flatten)\n",
    "# add lexical features\n",
    "lexical_input = Input(shape=(200,), name='lexical_input')\n",
    "x = merge([lexical_input, hidden], mode='concat')\n",
    "out = Dense(19,activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7994/7994 [==============================] - 3s - loss: 2.7094 - acc: 0.1709     \n",
      "Epoch 2/10\n",
      "7994/7994 [==============================] - 4s - loss: 2.7021 - acc: 0.1715     \n",
      "Epoch 3/10\n",
      "7994/7994 [==============================] - 4s - loss: 2.6943 - acc: 0.1759     \n",
      "Epoch 4/10\n",
      "7994/7994 [==============================] - 4s - loss: 2.6920 - acc: 0.1736     \n",
      "Epoch 5/10\n",
      "7994/7994 [==============================] - 4s - loss: 2.6902 - acc: 0.1748     \n",
      "Epoch 6/10\n",
      "7994/7994 [==============================] - 4s - loss: 2.6944 - acc: 0.1758     \n",
      "Epoch 7/10\n",
      "7994/7994 [==============================] - 4s - loss: 2.6933 - acc: 0.1726     \n",
      "Epoch 8/10\n",
      "7994/7994 [==============================] - 5s - loss: 2.6860 - acc: 0.1729     \n",
      "Epoch 9/10\n",
      "7994/7994 [==============================] - 5s - loss: 2.6852 - acc: 0.1754     \n",
      "Epoch 10/10\n",
      "7994/7994 [==============================] - 5s - loss: 2.6856 - acc: 0.1746     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f20bdf3bfd0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(input=[main_input,lexical_input],output=out)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit([X,lexical_feature], y, batch_size=20, nb_epoch=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
