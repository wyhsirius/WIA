{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peoject WIA\n",
    "#### Matthieu RÃ© and Yaohui WANG\n",
    "### Part1 Treatment of the dataset\n",
    "### Variables:\n",
    "1. final_sentence: the pure sentences in the dataset, no punctuations, no numbers, low letters\n",
    "2. final_couple: the nouns marks in the sentence\n",
    "3. list_index: the locals of the nous in the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"SemEval2010_task8_training/TRAIN_FILE.TXT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the train data line by line \n",
    "# store in a list\n",
    "def readinFile(f):\n",
    "    filelist = []\n",
    "    line = f.readline()\n",
    "    filelist.append(line)\n",
    "    while line:\n",
    "        line = f.readline()\n",
    "        filelist.append(line)\n",
    "    return filelist\n",
    "\n",
    "filelist = readinFile(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using 3 lists to store 3 elements\n",
    "# list_sentence: each sentence in the list\n",
    "# list_class: each class in the list\n",
    "# list comment: each comment in the list\n",
    "def separateFilelist(l):\n",
    "    list_sentence = []\n",
    "    list_class = []\n",
    "    list_comment = []\n",
    "    for i in range(len(l)):\n",
    "        if (i + 1) % 4 == 1:\n",
    "            list_sentence.append(l[i])\n",
    "        if (i + 1) % 4 == 2:\n",
    "            list_class.append(l[i])\n",
    "        if (i + 1) % 4 == 3:\n",
    "            list_comment.append(l[i])\n",
    "    # to remove the '' in the end, no use\n",
    "    list_sentence.remove('')\n",
    "    return list_sentence,list_class,list_comment\n",
    "\n",
    "list_sentence,list_class,list_comment = separateFilelist(filelist)\n",
    "\n",
    "# here we delete the sentences which give us noises\n",
    "# index: 212,2739,4218,4611,4783,6372\n",
    "\n",
    "list_sentence.remove(list_sentence[212])\n",
    "list_sentence.remove(list_sentence[2738])\n",
    "list_sentence.remove(list_sentence[4216])\n",
    "list_sentence.remove(list_sentence[4608])\n",
    "list_sentence.remove(list_sentence[4779])\n",
    "list_sentence.remove(list_sentence[6367])\n",
    "\n",
    "list_class.remove(list_class[212])\n",
    "list_class.remove(list_class[2738])\n",
    "list_class.remove(list_class[4216])\n",
    "list_class.remove(list_class[4608])\n",
    "list_class.remove(list_class[4779])\n",
    "list_class.remove(list_class[6367])\n",
    "\n",
    "list_comment.remove(list_comment[212])\n",
    "list_comment.remove(list_comment[2738])\n",
    "list_comment.remove(list_comment[4216])\n",
    "list_comment.remove(list_comment[4608])\n",
    "list_comment.remove(list_comment[4779])\n",
    "list_comment.remove(list_comment[6367])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the sentences is:  7994\n",
      "The length of the classes is:  7994\n",
      "The length of the comments is:  7994\n"
     ]
    }
   ],
   "source": [
    "def sizeoftrain(list_sentence,list_class,list_comment):\n",
    "    print(\"The length of the sentences is: \",len(list_sentence))\n",
    "    print(\"The length of the classes is: \",len(list_class))\n",
    "    print(\"The length of the comments is: \",len(list_comment))\n",
    "    \n",
    "sizeoftrain(list_sentence,list_class,list_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sirius/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /home/sirius/anaconda3/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def getNouns(list_sentence):\n",
    "    l_noun = []\n",
    "    for sentence in list_sentence:\n",
    "        flag = 0\n",
    "        l1 = ''\n",
    "        l2 = ''\n",
    "        couple_noun = []\n",
    "        for i in range(len(sentence)):\n",
    "            if (sentence[i] == '<') and (sentence[i+1] == 'e') and (sentence[i+2] == '1'):\n",
    "                flag = 1\n",
    "            if (sentence[i] == '>') and (sentence[i-1] == '1') and (sentence[i-2] == 'e') and (sentence[i-3] == '/'):\n",
    "                flag = 0\n",
    "            if (sentence[i] == '<') and (sentence[i+1] == 'e') and (sentence[i+2] == '2'):\n",
    "                flag = 2\n",
    "            if (sentence[i] == '>') and (sentence[i-1] == '2') and (sentence[i-2] == 'e') and (sentence[i-3] == '/'):\n",
    "                flag = 0\n",
    "\n",
    "            if flag == 1:\n",
    "                l1 += sentence[i]\n",
    "            if flag == 2:\n",
    "                l2 += sentence[i]\n",
    "        noun1_b = BeautifulSoup(l1)\n",
    "        noun2_b = BeautifulSoup(l2)\n",
    "        noun1 = noun1_b.get_text()\n",
    "        noun2 = noun2_b.get_text()\n",
    "        couple_noun.append(noun1)\n",
    "        couple_noun.append(noun2)\n",
    "        l_noun.append(couple_noun)\n",
    "    return l_noun\n",
    "\n",
    "l_noun = getNouns(list_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# replace the space in the marked nous into #, and replace these nous in the sentences\n",
    "# example: WANG Yaohui -> WANG#Yaohui\n",
    "\n",
    "import nltk.data\n",
    "import re\n",
    "\n",
    "def replace_the_space(l_noun):\n",
    "    l_new_noun = []\n",
    "    for couple in l_noun:\n",
    "        c = (couple[0].replace(' ','#'),couple[1].replace(' ','#'))\n",
    "        l_new_noun.append(c)\n",
    "    return l_new_noun\n",
    "\n",
    "def replace_original_noun(list_sentence,l_new_noun):\n",
    "    l_new_sentence = []\n",
    "    for sentence,couple in zip(list_sentence,l_new_noun):\n",
    "        left = couple[0].replace('#',' ')\n",
    "        right = couple[1].replace('#',' ')\n",
    "        sentence = re.sub(\"<e1>\" + left + \"</e1>\",couple[0],sentence)\n",
    "        sentence = re.sub(\"<e2>\" + right + \"</e2>\",couple[1],sentence)\n",
    "        l_new_sentence.append(sentence)\n",
    "    return l_new_sentence\n",
    "\n",
    "def get_the_new_sentence(list_sentence,l_noun):\n",
    "    final_sentence = []\n",
    "    \n",
    "    l_new_noun = replace_the_space(l_noun)\n",
    "    l_new_sentence = replace_original_noun(list_sentence,l_new_noun)\n",
    "    \n",
    "    for l in l_new_sentence:\n",
    "        sentence_text = re.sub(\"[^a-zA-Z#-]\",\" \",l)\n",
    "        words = sentence_text.lower().split()\n",
    "        final_sentence.append(words)\n",
    "    return final_sentence\n",
    "\n",
    "def get_the_final_couple(l_noun):\n",
    "    new_couple = replace_the_space(l_noun)\n",
    "    final_couple = []\n",
    "    for couple in new_couple:\n",
    "        c = (couple[0].lower(),couple[1].lower())\n",
    "        final_couple.append(c)\n",
    "    return final_couple\n",
    "\n",
    "# final sentences we need\n",
    "final_sentence = get_the_new_sentence(list_sentence,l_noun)\n",
    "# final marked nouns we need\n",
    "final_couple = get_the_final_couple(l_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_location(final_sentence,final_couple):\n",
    "    list_index = []\n",
    "    for couple,sentence in zip(final_couple,final_sentence):\n",
    "        #print(sentence)\n",
    "        #print(couple[0] + couple[1])\n",
    "        noun_couple = (sentence.index(couple[0]),sentence.index(couple[1]))\n",
    "        list_index.append(noun_couple)\n",
    "    return list_index\n",
    "\n",
    "# location of marked nouns in the sentences\n",
    "list_index = get_location(final_sentence, final_couple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-08 16:43:06,336: INFO: collecting all words and their counts\n",
      "2017-01-08 16:43:06,336: INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-01-08 16:43:06,378: INFO: collected 19818 word types from a corpus of 136793 raw words and 7994 sentences\n",
      "2017-01-08 16:43:06,379: INFO: Loading a fresh vocabulary\n",
      "2017-01-08 16:43:06,390: INFO: min_count=40 retains 279 unique words (1% of original 19818, drops 19539)\n",
      "2017-01-08 16:43:06,390: INFO: min_count=40 leaves 73735 word corpus (53% of original 136793, drops 63058)\n",
      "2017-01-08 16:43:06,392: INFO: deleting the raw counts dictionary of 19818 items\n",
      "2017-01-08 16:43:06,394: INFO: sample=0.0001 downsamples 279 most-common words\n",
      "2017-01-08 16:43:06,395: INFO: downsampling leaves estimated 11028 word corpus (15.0% of prior 73735)\n",
      "2017-01-08 16:43:06,396: INFO: estimated required memory for 279 words and 400 dimensions: 1032300 bytes\n",
      "2017-01-08 16:43:06,399: INFO: resetting layer weights\n",
      "2017-01-08 16:43:06,404: INFO: training model with 8 workers on 279 vocabulary and 400 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-01-08 16:43:06,405: INFO: expecting 7994 sentences, matching count from corpus used for vocabulary survey\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-08 16:43:06,581: INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2017-01-08 16:43:06,583: INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2017-01-08 16:43:06,585: INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2017-01-08 16:43:06,589: INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2017-01-08 16:43:06,591: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2017-01-08 16:43:06,593: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-08 16:43:06,594: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-08 16:43:06,595: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-08 16:43:06,595: INFO: training on 683965 raw words (54888 effective words) took 0.2s, 300263 effective words/s\n",
      "2017-01-08 16:43:06,596: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# apply word2vec on our final sentences list\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s', level = logging.INFO)\n",
    "\n",
    "num_features = 400\n",
    "min_word_count = 1\n",
    "num_workers = 8\n",
    "context = 10\n",
    "downsampling = 0.0001\n",
    "\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(final_sentence, workers = num_workers, size = num_features, min_count = min_word_count, window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-08 16:43:08,659: INFO: precomputing L2-norms of word weight vectors\n",
      "2017-01-08 16:43:08,662: INFO: saving Word2Vec object under wia, separately None\n",
      "2017-01-08 16:43:08,662: INFO: not storing attribute syn0norm\n",
      "2017-01-08 16:43:08,663: INFO: not storing attribute cum_table\n",
      "2017-01-08 16:43:08,671: INFO: saved wia\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace = True)\n",
    "\n",
    "model_name = \"wia\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man waman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('that', 0.9998661279678345),\n",
       " ('we', 0.9998630285263062),\n",
       " ('their', 0.9998619556427002),\n",
       " ('to', 0.9998538494110107),\n",
       " ('and', 0.9998519420623779),\n",
       " ('so', 0.9998518228530884),\n",
       " ('i', 0.9998513460159302),\n",
       " ('from', 0.9998495578765869),\n",
       " ('it', 0.9998487830162048),\n",
       " ('had', 0.9998475313186646)]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
