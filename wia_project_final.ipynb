{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peoject WIA\n",
    "#### Matthieu RÃ© and Yaohui WANG\n",
    "### Part1 Treatment of the dataset\n",
    "### Variables:\n",
    "1. final_sentence: the pure sentences in the dataset, no punctuations, no numbers, low letters\n",
    "2. final_couple: the nouns marks in the sentence\n",
    "3. list_index: the locals of the nous in the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"SemEval2010_task8_training/TRAIN_FILE.TXT\")\n",
    "\n",
    "# read the train data line by line \n",
    "# store in a list\n",
    "def readinFile(f):\n",
    "    filelist = []\n",
    "    line = f.readline()\n",
    "    filelist.append(line)\n",
    "    while line:\n",
    "        line = f.readline()\n",
    "        filelist.append(line)\n",
    "    return filelist\n",
    "\n",
    "filelist = readinFile(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using 3 lists to store 3 elements\n",
    "# list_sentence: each sentence in the list\n",
    "# list_class: each class in the list\n",
    "# list comment: each comment in the list\n",
    "def separateFilelist(l):\n",
    "    list_sentence = []\n",
    "    list_class = []\n",
    "    list_comment = []\n",
    "    for i in range(len(l)):\n",
    "        if (i + 1) % 4 == 1:\n",
    "            list_sentence.append(l[i])\n",
    "        if (i + 1) % 4 == 2:\n",
    "            list_class.append(l[i])\n",
    "        if (i + 1) % 4 == 3:\n",
    "            list_comment.append(l[i])\n",
    "    # to remove the '' in the end, no use\n",
    "    list_sentence.remove('')\n",
    "    return list_sentence,list_class,list_comment\n",
    "\n",
    "list_sentence,list_class,list_comment = separateFilelist(filelist)\n",
    "\n",
    "# here we delete the sentences which give us noises\n",
    "# index: 212,2739,4218,4611,4783,6372\n",
    "\n",
    "list_sentence.remove(list_sentence[212])\n",
    "list_sentence.remove(list_sentence[2738])\n",
    "list_sentence.remove(list_sentence[4216])\n",
    "list_sentence.remove(list_sentence[4608])\n",
    "list_sentence.remove(list_sentence[4779])\n",
    "list_sentence.remove(list_sentence[6367])\n",
    "\n",
    "list_class.remove(list_class[212])\n",
    "list_class.remove(list_class[2738])\n",
    "list_class.remove(list_class[4216])\n",
    "list_class.remove(list_class[4608])\n",
    "list_class.remove(list_class[4779])\n",
    "list_class.remove(list_class[6367])\n",
    "\n",
    "list_comment.remove(list_comment[212])\n",
    "list_comment.remove(list_comment[2738])\n",
    "list_comment.remove(list_comment[4216])\n",
    "list_comment.remove(list_comment[4608])\n",
    "list_comment.remove(list_comment[4779])\n",
    "list_comment.remove(list_comment[6367])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the sentences is:  7994\n",
      "The length of the classes is:  7994\n",
      "The length of the comments is:  7994\n"
     ]
    }
   ],
   "source": [
    "def sizeoftrain(list_sentence,list_class,list_comment):\n",
    "    print(\"The length of the sentences is: \",len(list_sentence))\n",
    "    print(\"The length of the classes is: \",len(list_class))\n",
    "    print(\"The length of the comments is: \",len(list_comment))\n",
    "    \n",
    "sizeoftrain(list_sentence,list_class,list_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sirius/anaconda3/envs/theano/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /home/sirius/anaconda3/envs/theano/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def getNouns(list_sentence):\n",
    "    l_noun = []\n",
    "    for sentence in list_sentence:\n",
    "        flag = 0\n",
    "        l1 = ''\n",
    "        l2 = ''\n",
    "        couple_noun = []\n",
    "        for i in range(len(sentence)):\n",
    "            if (sentence[i] == '<') and (sentence[i+1] == 'e') and (sentence[i+2] == '1'):\n",
    "                flag = 1\n",
    "            if (sentence[i] == '>') and (sentence[i-1] == '1') and (sentence[i-2] == 'e') and (sentence[i-3] == '/'):\n",
    "                flag = 0\n",
    "            if (sentence[i] == '<') and (sentence[i+1] == 'e') and (sentence[i+2] == '2'):\n",
    "                flag = 2\n",
    "            if (sentence[i] == '>') and (sentence[i-1] == '2') and (sentence[i-2] == 'e') and (sentence[i-3] == '/'):\n",
    "                flag = 0\n",
    "\n",
    "            if flag == 1:\n",
    "                l1 += sentence[i]\n",
    "            if flag == 2:\n",
    "                l2 += sentence[i]\n",
    "        noun1_b = BeautifulSoup(l1)\n",
    "        noun2_b = BeautifulSoup(l2)\n",
    "        noun1 = noun1_b.get_text()\n",
    "        noun2 = noun2_b.get_text()\n",
    "        couple_noun.append(noun1)\n",
    "        couple_noun.append(noun2)\n",
    "        l_noun.append(couple_noun)\n",
    "    return l_noun\n",
    "\n",
    "l_noun = getNouns(list_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# replace the space in the marked nous into #, and replace these nous in the sentences\n",
    "# example: WANG Yaohui -> WANG#Yaohui\n",
    "\n",
    "import nltk.data\n",
    "import re\n",
    "\n",
    "def replace_the_space(l_noun):\n",
    "    l_new_noun = []\n",
    "    for couple in l_noun:\n",
    "        c = (couple[0].replace(' ','#'),couple[1].replace(' ','#'))\n",
    "        l_new_noun.append(c)\n",
    "    return l_new_noun\n",
    "\n",
    "def replace_original_noun(list_sentence,l_new_noun):\n",
    "    l_new_sentence = []\n",
    "    for sentence,couple in zip(list_sentence,l_new_noun):\n",
    "        left = couple[0].replace('#',' ')\n",
    "        right = couple[1].replace('#',' ')\n",
    "        sentence = re.sub(\"<e1>\" + left + \"</e1>\",couple[0],sentence)\n",
    "        sentence = re.sub(\"<e2>\" + right + \"</e2>\",couple[1],sentence)\n",
    "        l_new_sentence.append(sentence)\n",
    "    return l_new_sentence\n",
    "\n",
    "def get_the_new_sentence(list_sentence,l_noun):\n",
    "    final_sentence = []\n",
    "    \n",
    "    l_new_noun = replace_the_space(l_noun)\n",
    "    l_new_sentence = replace_original_noun(list_sentence,l_new_noun)\n",
    "    \n",
    "    for l in l_new_sentence:\n",
    "        sentence_text = re.sub(\"[^a-zA-Z#-]\",\" \",l)\n",
    "        words = sentence_text.lower().split()\n",
    "        final_sentence.append(words)\n",
    "    return final_sentence\n",
    "\n",
    "def get_the_final_couple(l_noun):\n",
    "    new_couple = replace_the_space(l_noun)\n",
    "    final_couple = []\n",
    "    for couple in new_couple:\n",
    "        c = (couple[0].lower(),couple[1].lower())\n",
    "        final_couple.append(c)\n",
    "    return final_couple\n",
    "\n",
    "# final sentences we need\n",
    "final_sentence = get_the_new_sentence(list_sentence,l_noun)\n",
    "# final marked nouns we need\n",
    "final_couple = get_the_final_couple(l_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_location(final_sentence,final_couple):\n",
    "    list_index = []\n",
    "    for couple,sentence in zip(final_couple,final_sentence):\n",
    "        #print(sentence)\n",
    "        #print(couple[0] + couple[1])\n",
    "        noun_couple = (sentence.index(couple[0]),sentence.index(couple[1]))\n",
    "        list_index.append(noun_couple)\n",
    "    return list_index\n",
    "\n",
    "# location of marked nouns in the sentences\n",
    "list_index = get_location(final_sentence, final_couple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-09 01:09:40,533: WARNING: consider setting layer size to a multiple of 4 for greater performance\n",
      "2017-01-09 01:09:40,535: INFO: collecting all words and their counts\n",
      "2017-01-09 01:09:40,536: INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-01-09 01:09:40,574: INFO: collected 19818 word types from a corpus of 136793 raw words and 7994 sentences\n",
      "2017-01-09 01:09:40,575: INFO: Loading a fresh vocabulary\n",
      "2017-01-09 01:09:40,692: INFO: min_count=1 retains 19818 unique words (100% of original 19818, drops 0)\n",
      "2017-01-09 01:09:40,693: INFO: min_count=1 leaves 136793 word corpus (100% of original 136793, drops 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-09 01:09:40,762: INFO: deleting the raw counts dictionary of 19818 items\n",
      "2017-01-09 01:09:40,764: INFO: sample=0.0001 downsamples 330 most-common words\n",
      "2017-01-09 01:09:40,765: INFO: downsampling leaves estimated 79036 word corpus (57.8% of prior 136793)\n",
      "2017-01-09 01:09:40,766: INFO: estimated required memory for 19818 words and 50 dimensions: 17836200 bytes\n",
      "2017-01-09 01:09:40,830: INFO: resetting layer weights\n",
      "2017-01-09 01:09:41,100: INFO: training model with 8 workers on 19818 vocabulary and 50 features, using sg=0 hs=0 sample=0.0001 negative=5 window=10\n",
      "2017-01-09 01:09:41,101: INFO: expecting 7994 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-01-09 01:09:41,109: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,109: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,110: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,110: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,110: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,110: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,116: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,117: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,155: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,166: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,182: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,192: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,193: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,207: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,216: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,225: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,245: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,259: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,268: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,269: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,293: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,296: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,308: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,315: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,334: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,334: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,343: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,353: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,361: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,378: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,392: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,392: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,398: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,400: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,414: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,425: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,434: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,435: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,451: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,460: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,479: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,479: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,487: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,497: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,507: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,507: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,526: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,537: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,555: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,555: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,568: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,580: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,581: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,586: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,595: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,605: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,626: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,633: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,642: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,654: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,662: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,669: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,677: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,687: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,690: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,708: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,708: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,724: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,740: INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2017-01-09 01:09:41,741: WARNING: direct access to vocab will not be supported in future gensim releases, please use model.wv.vocab\n",
      "2017-01-09 01:09:41,767: INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2017-01-09 01:09:41,774: INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2017-01-09 01:09:41,776: INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2017-01-09 01:09:41,778: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2017-01-09 01:09:41,782: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2017-01-09 01:09:41,783: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2017-01-09 01:09:41,788: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2017-01-09 01:09:41,789: INFO: training on 683965 raw words (395229 effective words) took 0.7s, 582418 effective words/s\n",
      "2017-01-09 01:09:41,789: WARNING: under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "# apply word2vec on our final sentences list\n",
    "from gensim import models\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s', level = logging.INFO)\n",
    "\n",
    "num_features = 50\n",
    "min_word_count = 1\n",
    "num_workers = 8\n",
    "context = 10\n",
    "downsampling = 0.0001\n",
    "print(\"Training model...\")\n",
    "\n",
    "model = models.word2vec.Word2Vec(final_sentence, workers = num_workers, size = num_features, min_count = min_word_count, window = context, sample = downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-09 01:09:45,381: INFO: precomputing L2-norms of word weight vectors\n",
      "2017-01-09 01:09:45,517: INFO: saving Word2Vec object under wia, separately None\n",
      "2017-01-09 01:09:45,518: WARNING: direct access to syn0norm will not be supported in future gensim releases, please use model.wv.syn0norm\n",
      "2017-01-09 01:09:45,519: WARNING: direct access to syn0norm will not be supported in future gensim releases, please use model.wv.syn0norm\n",
      "2017-01-09 01:09:45,520: WARNING: direct access to syn0norm will not be supported in future gensim releases, please use model.wv.syn0norm\n",
      "2017-01-09 01:09:45,520: INFO: not storing attribute cum_table\n",
      "2017-01-09 01:09:45,521: INFO: not storing attribute syn0norm\n",
      "2017-01-09 01:09:45,622: WARNING: direct access to syn0norm will not be supported in future gensim releases, please use model.wv.syn0norm\n",
      "2017-01-09 01:09:45,623: INFO: saved wia\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace = True)\n",
    "\n",
    "model_name = \"wia\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'child'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man waman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# L1 L2 L3 L4\n",
    "def get_lexical_feature(final_couple,list_sentence,final_sentence):\n",
    "    lexical_feature = []\n",
    "    for couple,index,sentence in zip(final_couple,list_index,final_sentence):\n",
    "\n",
    "        noun1_v = model[couple[0]]\n",
    "        noun2_v = model[couple[1]]\n",
    "    \n",
    "        if index[0] == 0:\n",
    "            noun3_v = model[sentence[index[0]+1]] / 2 \n",
    "        if index[0] == (len(sentence)-1):\n",
    "            noun3_v = model[sentence[index[0]-1]] / 2\n",
    "        else:\n",
    "            noun3_v = (model[sentence[index[0]-1]] + model[sentence[index[0]+1]]) / 2\n",
    "        \n",
    "        if index[1] == 0:\n",
    "            noun4_v = model[sentence[index[1]+1]] / 2 \n",
    "        if index[1] == (len(sentence)-1):\n",
    "            noun4_v = model[sentence[index[1]-1]] / 2\n",
    "        else:\n",
    "            noun4_v = (model[sentence[index[1]-1]] + model[sentence[index[1]+1]]) / 2\n",
    "    \n",
    "        word_feature = np.concatenate((noun1_v,noun2_v,noun3_v,noun4_v))\n",
    "        lexical_feature.append(word_feature)\n",
    "    return np.array(lexical_feature)\n",
    "\n",
    "lexical_feature = get_lexical_feature(final_couple,list_sentence,final_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7994, 200)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Level Feature\n",
    "#### Part 1\n",
    "get the [WF, PF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the beginning and the end \n",
    "xs = np.zeros(50)\n",
    "xe = np.zeros(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the [WF, PF]\n",
    "def get_the_sentence_feature(final_sentence,list_index):\n",
    "    window = 3\n",
    "    list_sentence_feature = []\n",
    "    for sentence,position in zip(final_sentence,list_index):\n",
    "        wf_l = []\n",
    "        for index in range(len(sentence)):\n",
    "            if index == 0:\n",
    "                wf = np.concatenate((xs, model[sentence[index]], model[sentence[index+1]], [index-position[0]], [index-position[1]]))\n",
    "            if index == (len(sentence)-1):\n",
    "                wf = np.concatenate((model[sentence[index-1]], model[sentence[index]], xe, [index-position[0]], [index-position[1]]))\n",
    "            else:\n",
    "                wf = np.concatenate((model[sentence[index-1]], model[sentence[index]], model[sentence[index+1]], [index-position[0]], [index-position[1]]))\n",
    "            wf_l.append(wf)\n",
    "        list_sentence_feature.append(wf_l)\n",
    "    return list_sentence_feature\n",
    "\n",
    "list_sentence_feature = get_the_sentence_feature(final_sentence,list_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_sentence_feature[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2\n",
    "##### Training the model CNN and get the sentence level feature\n",
    "Layer 1\n",
    "\n",
    "Activate Function: Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_f(z):\n",
    "    res_l1 = z.max(axis=1)\n",
    "    return res_l1\n",
    "\n",
    "def layer_1(W1,x):\n",
    "    a = np.dot(W1,np.array(x).transpose())\n",
    "    m = max_f(a)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.ifelse import ifelse\n",
    "import numpy as np\n",
    "\n",
    "x1 = T.matrix('x1')\n",
    "w1 = theano.shared(np.ones([200,152]))\n",
    "\n",
    "z1 = T.dot(w1,x1.transpose())\n",
    "a1 = z1.max(axis=1)\n",
    "\n",
    "layer1 = theano.function([x1],a1)\n",
    "\n",
    "inputs = list_sentence_feature\n",
    "\n",
    "list_out = []\n",
    "for i in range(len(inputs)):\n",
    "    t = inputs[i]\n",
    "    out = layer1(t)\n",
    "    list_out.append(out)\n",
    "\n",
    "list_out = np.array(list_out)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer 2\n",
    "\n",
    "Activate Function: tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tanh_f(z):\n",
    "    res_l2 = np.tanh(z)\n",
    "    return res_l2\n",
    "\n",
    "def layer_2(W2,x):\n",
    "    a = np.dot(W2,x)\n",
    "    m = np.tanh(a)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n1 = 200\n",
    "# n2 = 100\n",
    "def cnn_sentence_feature(n1,n2,list_sentence_feature):\n",
    "    n0 = 152 # the length of each word feature vecteur\n",
    "    W1 = np.ones((n1,n0))\n",
    "    l1_output = []\n",
    "    \n",
    "    W2 = np.ones((n2,n1))\n",
    "    l2_output = []\n",
    "    \n",
    "    for x in list_sentence_feature:\n",
    "        # first layer\n",
    "        m1 = layer_1(W1,x)\n",
    "        l1_output.append(m1)\n",
    "        \n",
    "        # second layer\n",
    "        m2 = layer_2(W2,m1)\n",
    "        l2_output.append(m2)\n",
    "    return l1_output,l2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1_output, l2_output = cnn_sentence_feature(200,100,list_sentence_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer output \n",
    "\n",
    "Activate Function: softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lexical level feature + sentence level feature\n",
    "list_feature_final = []\n",
    "for wf,sf in zip(lexical_feature,l2_output):\n",
    "    feature_final = np.concatenate((sf,wf))\n",
    "    list_feature_final.append(feature_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    p = np.max(z,0)\n",
    "    return np.exp(z-p) / np.sum(np.exp(z-p), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer_3(W3,x):\n",
    "    a = np.dot(W3,x)\n",
    "    m = softmax(a)\n",
    "    return m"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:theano]",
   "language": "python",
   "name": "conda-env-theano-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
